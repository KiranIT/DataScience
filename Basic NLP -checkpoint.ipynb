{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-37fb6da946c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#import gensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#nltk.download()\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#import gensim\n",
    "#from gensim import corpora\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('Tweets_NLP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"ABCG\"\n",
    "x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #studiolife #aislife #requires #passion #dedic...\n",
       "1    @user #white #supremacists want everyone to se...\n",
       "2    safe ways to heal your #acne!! #altwaystoheal ...\n",
       "3    is the hp and the cursed child book up for res...\n",
       "4    3rd #bihday to my amazing, hilarious #nephew e...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 Text pre-processing\n",
    "\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    studiolife aislife requires passion dedication...\n",
       "1    user white supremacists want everyone to see t...\n",
       "2    safe ways to heal your acne altwaystoheal heal...\n",
       "3    is the hp and the cursed child book up for res...\n",
       "4    3rd bihday to my amazing hilarious nephew eli ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'] = train['tweet'].str.replace('[^\\w\\s]','')\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = [\"safe\", \"nephew\",\"is\",\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bcf094d29481>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2c714e1ea0d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#stop = stopwords.words('english')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#stop = stopwords.words('english')\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"is\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"ps\"\n",
    "TextBlob(x).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a09a3cc527cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "train['tweet'][5].apply(lambda txt: ''.join(textblob.TextBlob(txt).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-374331912993>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "train['tweet'][5].apply(lambda x:str(TextBlob(x).correct()))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['user', 'white', 'supremacists', 'want', 'everyone', 'see', 'new', 'â', 'birdsâ', 'movie', 'â', 'hereâs'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "TextBlob(train['tweet'][1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"studiolife aislife requires passion dedication\"\n",
    "x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    studiolif aislif requir passion dedic willpow ...\n",
       "1    user white supremacist want everyon see new â ...\n",
       "2                 way heal acn altwaystoh healthi heal\n",
       "3    hp curs child book reserv alreadi ye ððð harry...\n",
       "4    3rd bihday amaz hilari eli ahmir uncl dave lov...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming \n",
    "\n",
    "st = PorterStemmer()\n",
    "train['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    studiolife aislife requires passion dedication...\n",
       "1    user white supremacist want everyone see new â...\n",
       "2          way heal acne altwaystoheal healthy healing\n",
       "3    hp cursed child book reservation already yes ð...\n",
       "4    3rd bihday amazing hilarious eli ahmir uncle d...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N gram\n",
    "\n",
    "#TextBlob(train['tweet'][0]).ngrams(2)\n",
    "TextBlob(\"product not good\").ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"I love my job\" \n",
    "test2 = \"I hate love\"\n",
    "test3 = \"average\"\n",
    "\n",
    "test4 = \"Marriage is a romance in which the heroine dies in the first chapter\"\n",
    "test5 = \"Honesty is the best policy -- when there is money in it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.25, subjectivity=0.3333333333333333)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(test4)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>user white supremacist want everyone see new â...</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>way heal acne altwaystoheal healthy healing</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>hp cursed child book reservation already yes ð...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd bihday amazing hilarious eli ahmir uncle d...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31968</td>\n",
       "      <td>choose momtips</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31969</td>\n",
       "      <td>something inside dy ððâ eye ness smokeyeyes ti...</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31970</td>\n",
       "      <td>finishedtattooinkedinkloveitâï âïâïâïâï thanks...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31971</td>\n",
       "      <td>user user user never understand dad left young...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31972</td>\n",
       "      <td>delicious food lovelife capetown mannaepicure ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31973</td>\n",
       "      <td>1000dayswasted narcosis infinite ep make aware...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31974</td>\n",
       "      <td>one world greatest spoing event lemans24 teamaudi</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31975</td>\n",
       "      <td>half way website allgoingwell</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31976</td>\n",
       "      <td>good food good life enjoy ðððððð called garlic...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31977</td>\n",
       "      <td>ill stand behind guncontrolplease senselesssho...</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31978</td>\n",
       "      <td>atei ate ateðð jamaisasthi fish curry prawn hi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31979</td>\n",
       "      <td>user got user limited edition rain shine set t...</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31980</td>\n",
       "      <td>amp love amp hug amp kiss keep baby parenting ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31981</td>\n",
       "      <td>ððð girl sun fave london united kingdom</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31982</td>\n",
       "      <td>thought factory bbc neutrality right wing fasc...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31983</td>\n",
       "      <td>hey guy tommorow last day exam im happy yay</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31984</td>\n",
       "      <td>user user user levyrroni recuerdos memoriesðâð...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31985</td>\n",
       "      <td>mind like ððð½ð body like ðððµð½ sleepy stilla...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31986</td>\n",
       "      <td>never entire life</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31987</td>\n",
       "      <td>check twitterww trend trending worldwide 1114 ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31988</td>\n",
       "      <td>thought saw mermaid ceegee smcr inshot girl cu...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31989</td>\n",
       "      <td>chick get fucked hottest naked lady</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31990</td>\n",
       "      <td>happy bday lucyââð xoxo love beautiful pizza i...</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31991</td>\n",
       "      <td>haroldfriday weekend filled sunbeam everyone h...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31992</td>\n",
       "      <td>user user tried nothing try know loved 2 3rd l...</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>32033</td>\n",
       "      <td>lipolight helped shape help shape learn user l...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>32034</td>\n",
       "      <td>ahhh leaving user tomorrowððð¼ðð user eforest2...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>32035</td>\n",
       "      <td>blessed hear morning chorus good morning peopl...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>32036</td>\n",
       "      <td>aunti mi find hilarious post nigeria enough tr...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>32037</td>\n",
       "      <td>user 6 chapter left book amp nearly complete k...</td>\n",
       "      <td>-0.047917</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>32038</td>\n",
       "      <td>user thrilled working user coming month announ...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>32039</td>\n",
       "      <td>trying another way make sourdough bread one ma...</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>32040</td>\n",
       "      <td>cool old door window 5000 vendor 509 shoplocal...</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>32041</td>\n",
       "      <td>fathersday fatherday user team</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>32042</td>\n",
       "      <td>greatbritain eye sky rafredarrows fly past mal...</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>32043</td>\n",
       "      <td>user 8000 followerswe blessed love everyone su...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>32044</td>\n",
       "      <td>user user user user user lt feeding public tro...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>32045</td>\n",
       "      <td>website live avon makeup fresh look</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>32046</td>\n",
       "      <td>forecast look good weather across bolton</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>32047</td>\n",
       "      <td>user happy shootingðââïâðâðð love monicaðð swe...</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>32048</td>\n",
       "      <td>happy father day ððð father day fathersday dad...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>32049</td>\n",
       "      <td>stardivarius friday new history new outfit pos...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>32050</td>\n",
       "      <td>user final countdown official u publication ju...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>32051</td>\n",
       "      <td>happy trusted first knowâð lovelovelovebabies</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>32052</td>\n",
       "      <td>interview feat grandmaster flash ze lovely mes...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>32053</td>\n",
       "      <td>im getting hour work training im</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>32054</td>\n",
       "      <td>life right amazingð successful positive</td>\n",
       "      <td>0.420996</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>32055</td>\n",
       "      <td>social medium reality really get viual accepta...</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>32056</td>\n",
       "      <td>user ðª seven tomorrowðª southwestseason ðfina...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>32057</td>\n",
       "      <td>user user user user user ignorant amp ill info...</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>32058</td>\n",
       "      <td>great see look forward welcoming digme</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>32059</td>\n",
       "      <td>jackblair na horny hot naughty nasty slut youn...</td>\n",
       "      <td>-0.128571</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>32060</td>\n",
       "      <td>user user u take pour one big cup drink delivered</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>32061</td>\n",
       "      <td>ððððâïððð happy father day dad hope u great da...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>32062</td>\n",
       "      <td>working anatomy studyguide since 5 pm still do...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              tweet  sentiment  \\\n",
       "0   31963  studiolife aislife requires passion dedication...   0.000000   \n",
       "1   31964  user white supremacist want everyone see new â...   0.068182   \n",
       "2   31965        way heal acne altwaystoheal healthy healing   0.500000   \n",
       "3   31966  hp cursed child book reservation already yes ð...   0.500000   \n",
       "4   31967  3rd bihday amazing hilarious eli ahmir uncle d...   0.400000   \n",
       "5   31968                                     choose momtips   0.000000   \n",
       "6   31969  something inside dy ððâ eye ness smokeyeyes ti...  -0.250000   \n",
       "7   31970  finishedtattooinkedinkloveitâï âïâïâïâï thanks...   0.000000   \n",
       "8   31971  user user user never understand dad left young...   0.033333   \n",
       "9   31972  delicious food lovelife capetown mannaepicure ...   1.000000   \n",
       "10  31973  1000dayswasted narcosis infinite ep make aware...   0.050000   \n",
       "11  31974  one world greatest spoing event lemans24 teamaudi   1.000000   \n",
       "12  31975                      half way website allgoingwell  -0.166667   \n",
       "13  31976  good food good life enjoy ðððððð called garlic...   0.600000   \n",
       "14  31977  ill stand behind guncontrolplease senselesssho...  -0.450000   \n",
       "15  31978  atei ate ateðð jamaisasthi fish curry prawn hi...   0.000000   \n",
       "16  31979  user got user limited edition rain shine set t...  -0.071429   \n",
       "17  31980  amp love amp hug amp kiss keep baby parenting ...   0.500000   \n",
       "18  31981            ððð girl sun fave london united kingdom   0.000000   \n",
       "19  31982  thought factory bbc neutrality right wing fasc...   0.285714   \n",
       "20  31983        hey guy tommorow last day exam im happy yay   0.400000   \n",
       "21  31984  user user user levyrroni recuerdos memoriesðâð...   0.000000   \n",
       "22  31985  mind like ððð½ð body like ðððµð½ sleepy stilla...   0.000000   \n",
       "23  31986                                  never entire life   0.000000   \n",
       "24  31987  check twitterww trend trending worldwide 1114 ...   0.000000   \n",
       "25  31988  thought saw mermaid ceegee smcr inshot girl cu...   0.500000   \n",
       "26  31989                chick get fucked hottest naked lady  -0.300000   \n",
       "27  31990  happy bday lucyââð xoxo love beautiful pizza i...   0.716667   \n",
       "28  31991  haroldfriday weekend filled sunbeam everyone h...   0.450000   \n",
       "29  31992  user user tried nothing try know loved 2 3rd l...   0.366667   \n",
       "..    ...                                                ...        ...   \n",
       "70  32033  lipolight helped shape help shape learn user l...   0.000000   \n",
       "71  32034  ahhh leaving user tomorrowððð¼ðð user eforest2...   0.000000   \n",
       "72  32035  blessed hear morning chorus good morning peopl...   0.700000   \n",
       "73  32036  aunti mi find hilarious post nigeria enough tr...   0.100000   \n",
       "74  32037  user 6 chapter left book amp nearly complete k...  -0.047917   \n",
       "75  32038  user thrilled working user coming month announ...   0.600000   \n",
       "76  32039  trying another way make sourdough bread one ma...  -0.500000   \n",
       "77  32040  cool old door window 5000 vendor 509 shoplocal...   0.225000   \n",
       "78  32041                     fathersday fatherday user team   0.000000   \n",
       "79  32042  greatbritain eye sky rafredarrows fly past mal...   0.275000   \n",
       "80  32043  user 8000 followerswe blessed love everyone su...   0.350000   \n",
       "81  32044  user user user user user lt feeding public tro...   0.000000   \n",
       "82  32045                website live avon makeup fresh look   0.218182   \n",
       "83  32046           forecast look good weather across bolton   0.700000   \n",
       "84  32047  user happy shootingðââïâðâðð love monicaðð swe...   0.537500   \n",
       "85  32048  happy father day ððð father day fathersday dad...   0.650000   \n",
       "86  32049  stardivarius friday new history new outfit pos...   0.136364   \n",
       "87  32050  user final countdown official u publication ju...   0.000000   \n",
       "88  32051      happy trusted first knowâð lovelovelovebabies   0.525000   \n",
       "89  32052  interview feat grandmaster flash ze lovely mes...   0.500000   \n",
       "90  32053                   im getting hour work training im   0.000000   \n",
       "91  32054            life right amazingð successful positive   0.420996   \n",
       "92  32055  social medium reality really get viual accepta...   0.144444   \n",
       "93  32056  user ðª seven tomorrowðª southwestseason ðfina...   0.000000   \n",
       "94  32057  user user user user user ignorant amp ill info...  -0.400000   \n",
       "95  32058             great see look forward welcoming digme   0.800000   \n",
       "96  32059  jackblair na horny hot naughty nasty slut youn...  -0.128571   \n",
       "97  32060  user user u take pour one big cup drink delivered   0.000000   \n",
       "98  32061  ððððâïððð happy father day dad hope u great da...   0.700000   \n",
       "99  32062  working anatomy studyguide since 5 pm still do...   0.000000   \n",
       "\n",
       "   Sentiments  \n",
       "0     Neutral  \n",
       "1    Positive  \n",
       "2    Positive  \n",
       "3    Positive  \n",
       "4    Positive  \n",
       "5     Neutral  \n",
       "6    Negative  \n",
       "7     Neutral  \n",
       "8    Positive  \n",
       "9    Positive  \n",
       "10   Positive  \n",
       "11   Positive  \n",
       "12   Negative  \n",
       "13   Positive  \n",
       "14   Negative  \n",
       "15    Neutral  \n",
       "16   Negative  \n",
       "17   Positive  \n",
       "18    Neutral  \n",
       "19   Positive  \n",
       "20   Positive  \n",
       "21    Neutral  \n",
       "22    Neutral  \n",
       "23    Neutral  \n",
       "24    Neutral  \n",
       "25   Positive  \n",
       "26   Negative  \n",
       "27   Positive  \n",
       "28   Positive  \n",
       "29   Positive  \n",
       "..        ...  \n",
       "70    Neutral  \n",
       "71    Neutral  \n",
       "72   Positive  \n",
       "73   Positive  \n",
       "74   Negative  \n",
       "75   Positive  \n",
       "76   Negative  \n",
       "77   Positive  \n",
       "78    Neutral  \n",
       "79   Positive  \n",
       "80   Positive  \n",
       "81    Neutral  \n",
       "82   Positive  \n",
       "83   Positive  \n",
       "84   Positive  \n",
       "85   Positive  \n",
       "86   Positive  \n",
       "87    Neutral  \n",
       "88   Positive  \n",
       "89   Positive  \n",
       "90    Neutral  \n",
       "91   Positive  \n",
       "92   Positive  \n",
       "93    Neutral  \n",
       "94   Negative  \n",
       "95   Positive  \n",
       "96   Negative  \n",
       "97    Neutral  \n",
       "98   Positive  \n",
       "99    Neutral  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sentiments'] = np.where(train['sentiment']>0, 'Positive', np.where(train['sentiment']<-0, 'Negative', 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17197 entries, 0 to 17196\n",
      "Data columns (total 4 columns):\n",
      "id            17197 non-null int64\n",
      "tweet         17197 non-null object\n",
      "sentiment     17197 non-null float64\n",
      "Sentiments    17197 non-null object\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 537.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modelling\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "\n",
    "exampleArray = ['I love NLP and I will learn NLP in 2month']\n",
    "               \n",
    "\n",
    "def processContent():\n",
    "    try:\n",
    "        for item in exampleArray:\n",
    "            tokenized = nltk.word_tokenize(item)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    \n",
    "\n",
    "\n",
    "processContent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'I love NLP and I will learn NLP in 2month'\n",
    "\n",
    "# importing necessary packages \n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# tokenize the text\n",
    "\n",
    "tokens = sent_tokenize(txt) \n",
    "\n",
    "#Generate taggin for all the tokens\n",
    "\n",
    "for i in tokens: \n",
    "    words = nltk.word_tokenize(i) \n",
    "    words = [w for w in words if not w in stop_words]  \n",
    "  \n",
    "    #  POS-tagger.  \n",
    "    tags = nltk.pos_tag(words) \n",
    "\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tCC coordinating conjunction\n",
    "\tCD cardinal digit\n",
    "\tDT determiner\n",
    "\tEX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "\tFW foreign word\n",
    "\tIN preposition/subordinating conjunction\n",
    "\tJJ adjective 'big'\n",
    "\tJJR adjective, comparative 'bigger'\n",
    "\tJJS adjective, superlative 'biggest'\n",
    "\tLS list marker 1)\n",
    "\tMD modal could, will\n",
    "\tNN noun, singular 'desk'\n",
    "\tNNS noun plural 'desks'\n",
    "\tNNP proper noun, singular 'Harrison'\n",
    "\tNNPS proper noun, plural 'Americans'\n",
    "\tPDT predeterminer 'all the kids'\n",
    "\tPOS possessive ending parent's\n",
    "\tPRP personal pronoun I, he, she\n",
    "\tPRP$ possessive pronoun my, his, hers\n",
    "\tRB adverb very, silently,\n",
    "\tRBR adverb, comparative better\n",
    "\tRBS adverb, superlative best\n",
    "\tRP particle give up\n",
    "\tTO to go 'to' the store.\n",
    "\tUH interjection errrrrrrrm\n",
    "\tVB verb, base form take\n",
    "\tVBD verb, past tense took\n",
    "\tVBG verb, gerund/present participle taking\n",
    "\tVBN verb, past participle taken\n",
    "\tVBP verb, sing. present, non-3d take\n",
    "\tVBZ verb, 3rd person sing. present takes\n",
    "\tWDT wh-determiner which\n",
    "\tWP wh-pronoun who, what\n",
    "\tWP$ possessive wh-pronoun whose\n",
    "\tWRB wh-abverb where, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"I love NLP and I will learn NLP in 2month I am lazy\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode another document\n",
    "text2 = [\"i am lazy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer\n",
    "\n",
    "Unethical, unprofessional and mismanaged RE store. On the day of my interceptor delivery the delivery in-charge was giving me a vehicle with wrong chassis number. Luckily I compared it with the invoice and quickly found out that the chassis number on the vehicle is different.\n",
    "\n",
    "This would have led to multiple problems later related to registration and insurance (remember, insurance is created before delivery).\n",
    "\n",
    "Lately I have been following up with them for my vehicle registration and there is no proper response. There is this person by the name Ganesh whom we are supposed to call and that guy doesn't pick up the phone!\n",
    "\n",
    "These guys only care until the invoice is created. Once they got your money you are nobody to them! Buy your vehicle from elsewhere!\n",
    "18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity scoring¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = (\n",
    "\"The sky is blue\",\n",
    "\"The sun is bright\",\n",
    "\"The sun in the sky is bright\",\n",
    "\"We can see the shining sun, the bright sun\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidf_matrix[2], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "sent = \"Mark is studying at Stanford University in California\"\n",
    "x = ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic flags\n",
    "The basic flags are I, L, M, S, U, X:\n",
    "\n",
    "re.I: This flag is used for ignoring casing\n",
    "re.M: This flag is useful if you want to find patterns throughout multiple lines\n",
    "re.L: This flag is used to find a local dependent\n",
    "re.S: This flag is used to find dot matches\n",
    "re.U: This flag is used to work for unicode data\n",
    "re.X: This flag is used for writing regex in a more readable format\n",
    "We have mainly used re.I, re.M, re.L, and re.U flags.\n",
    "\n",
    "We are using the re.match() and re.search() functions. Both are used to find the patterns and then you can process them according to the requirements of your application.\n",
    "\n",
    "Let's look at the differences between re.match() and re.search():\n",
    "\n",
    "re.match(): This checks for a match of the string only at the beginning of the string. So, if it finds the pattern at the beginning of the input string then it returns the matched pattern, otherwise; it returns a noun.\n",
    "re.search(): This checks for a match of the string anywhere in the string. It finds all the occurrences of the pattern in the given input string or data.\n",
    "Refer to the code snippet given in Figure 4.13:\n",
    "\n",
    "\n",
    "Find the single occurrence of character a and b:\n",
    "Regex: [ab] \n",
    "    \n",
    "Find characters except a and b:\n",
    "Regex: [^ab] \n",
    "    \n",
    "Find the character range of a to z:\n",
    "Regex: [a-z] \n",
    "    \n",
    "Find range except to z:\n",
    "Regex: [^a-z] \n",
    "    \n",
    "Find all the characters a to z as well as A to Z:\n",
    "Regex: [a-zA-Z] \n",
    "    \n",
    "Any single character:\n",
    "Regex: . \n",
    "    \n",
    "Any whitespace character:\n",
    "Regex: \\s \n",
    "    \n",
    "Any non-whitespace character:\n",
    "Regex: \\S \n",
    "    \n",
    "Any digit:\n",
    "Regex: \\d \n",
    "    \n",
    "Any non-digit:\n",
    "Regex: \\D \n",
    "    \n",
    "Any non-words:\n",
    "Regex: \\W \n",
    "    \n",
    "Any words:\n",
    "Regex: \\w \n",
    "    \n",
    "Either match a or b:\n",
    "Regex: (a|b) \n",
    "    \n",
    "Occurrence of a is either zero or one:\n",
    "Regex: a? ; ? Matches  zero or one occurrence not more than 1 occurrence \n",
    "    \n",
    "Occurrence of a is zero time or more than that:\n",
    "Regex: a* ; * matches zero or more than that \n",
    "    \n",
    "Occurrence of a is one time or more than that:\n",
    "Regex: a+ ; + matches occurrences one or more that one time \n",
    "    \n",
    "Exactly match three occurrences of a:\n",
    "Regex: a{3} \n",
    "    \n",
    "Match simultaneous occurrences of a with 3 or more than 3:\n",
    "Regex: a{3,} \n",
    "    \n",
    "Match simultaneous occurrences of a between 3 to 6:\n",
    "Regex: a{3,6} \n",
    "    \n",
    "Starting of the string:\n",
    "Regex: ^ \n",
    "\n",
    "Ending of the string:\n",
    "Regex: $ \n",
    "    \n",
    "Match word boundary:\n",
    "Regex: \\b \n",
    "    \n",
    "Non-word boundary:\n",
    "Regex: \\B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Method\tFunctionality\n",
    "s.find(t)\tindex of first instance of string t inside s (-1 if not found)\n",
    "s.rfind(t)\tindex of last instance of string t inside s (-1 if not found)\n",
    "s.index(t)\tlike s.find(t) except it raises ValueError if not found\n",
    "s.rindex(t)\tlike s.rfind(t) except it raises ValueError if not found\n",
    "s.join(text)\tcombine the words of the text into a string using s as the glue\n",
    "s.split(t)\tsplit s into a list wherever a t is found (whitespace by default)\n",
    "s.splitlines()\tsplit s into a list of strings, one per line\n",
    "s.lower()\ta lowercased version of the string s\n",
    "s.upper()\tan uppercased version of the string s\n",
    "s.title()\ta titlecased version of the string s\n",
    "s.strip()\ta copy of s without leading or trailing whitespace\n",
    "s.replace(t, u)\treplace instances of t with u inside s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
